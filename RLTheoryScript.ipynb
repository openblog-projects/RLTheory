{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db8ecdf",
   "metadata": {},
   "source": [
    "**1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240901a3",
   "metadata": {},
   "source": [
    "1. From the three fields of machine learning we will look into RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32df3e",
   "metadata": {},
   "source": [
    "**2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a7588",
   "metadata": {},
   "source": [
    "1. For understanding what is happening in our game we first need to understand some RL fundamentals because the agent in our game uses the proximal policy optimization for learning.\n",
    "\n",
    "2. Lets look into the building blocks of RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1361ca",
   "metadata": {},
   "source": [
    "**3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703e02c",
   "metadata": {},
   "source": [
    "1. In our game we will create an agent.\n",
    "2. And an environment in which the agents acts.\n",
    "3. The agent collects based on the environment observations $O_t$. $t$ stands for time step. Time steps are intervalls in which the agent interacts with the environment.\n",
    "4. Based on this observations performs the agent actions $A_t$\n",
    "5. Through these actions the agent gets reward $R_t$\n",
    "6. Actions, observations and rewards all together are called history $H_t$\n",
    "7. From this interactions we can derive us the state at a certain time step $S_t$ which is the function of the history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6240db",
   "metadata": {},
   "source": [
    "**4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727295b6",
   "metadata": {},
   "source": [
    "1. With this knowledge we can proceed to markov decision processes or short MDP.\n",
    "2. A MDP is a tuple which contains of:\n",
    "3. $S=$\n",
    "4. The set space of all states from the first state and the current state $t$.\n",
    "5. $A=$\n",
    "6. Which is the action space with an set of all actions. \n",
    "7. The transition function $P=$\n",
    "8. Which returns at a given state and action the likelyhood of landing in a certain state $S_{t+1}$\n",
    "9. And the reward function $R$ which determines the received reward after transitioning from state $S_t$ with action $A_t$ to the state $S_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fc12c",
   "metadata": {},
   "source": [
    "**5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60302db4",
   "metadata": {},
   "source": [
    "1. These were all the steps which are necessary to understand the policy.\n",
    "2. The policy which we need to understand the proximal policy optimization is written as $\\pi$\n",
    "3. And gives the probability of taking an action from a state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aa285",
   "metadata": {},
   "source": [
    "**6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf5d5a",
   "metadata": {},
   "source": [
    "1. Now we can go over the proximal policy optimization, short ppo algorithm.\n",
    "2. This is the function which we consider for the PPO algorithm.\n",
    "3. Step for step\n",
    "4. $L^{\\text{CLIP}}(\\theta)$ is the policy loss with $\\theta$ as an input in the function which are usually the weights of an neural network.\n",
    "5. $\\hat{\\mathop{\\mathbb{E}}}_t$ is the weighted average of all possible outcomes for an timestep $t$.\n",
    "6. The minimum $\\min$ is the smallest value of the function.\n",
    "7. Next we have the ratio from the current time step $t$ between the current policy and the previous.\n",
    "8. $\\hat{A_t}=$ is the advantage for choosing an certain action from an certain state.\n",
    "9. $\\text{clip}$ limits the values in the function \n",
    "10. The function is clipped between $1-\\epsilon, 1+\\epsilon$ where $\\epsilon$ is mostly $0.2$. Clipping means that the value which is generated from the function cannot fall under the limit of $1-\\epsilon$ and cannot go above $1+\\epsilon$\n",
    "11. We can write the probability ratio also more compact.\n",
    "12. With this our function for the PPO algorithm is complete.\n",
    "13. We have the probability ratios - one is clipped.\n",
    "14. If the minimum from the probability ratios is positive the probability of taking that action increases.\n",
    "15. The average from all our results will give us the final policy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
